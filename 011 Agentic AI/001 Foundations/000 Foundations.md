## Generative PerTrained Transformer

Generative - its not searching an Index, rather it generates text
Pretrained - On some data
Transformer - Need a better understanding... of this..

## Under the hood - Attention is All you need

- Transformer Architecture

* Google Translate for example from English to Hindi Translation
* GP - Transformer - given a word (token) - it will predict the next work (token)
*

## Tokens!

- Numbers are converted to tokens which are essentially a number representation of the word
- All models have their own tokenizer!

## Vector Embeddings

- Vector Embeddings - give you semantic meaning to the words ...
- It's simply a representation of data points, including text, images, and other data types
  that capture their meaning and relationship

## Position Encoding

- Positional encoding is a technique in deep learning, especially Transformers, that adds information about the
  order or position of words (tokens) in a sequence

## Multi-Head Attention for Rich Context

- Intuition : looking at the same text in multiple simple ways and combining the results.
