## Deplying and running LLMs Locally

1. Ollama - Ollama is an open-source tool for running large language models (LLMs) locally on your machine. It simplifies downloading, managing, and executing models like Llama, Mistral, or Gemma directly on consumer hardware without cloud dependency.

2. Open WebUI - Open WebUI is an open-source, self-hosted web interface designed to provide a user-friendly, ChatGPT-like frontend for interacting with local LLMs, especially those running via Ollama. It transforms Ollama's command-line experience into a modern, browser-based dashboard with real-time chat, model management, and advanced features, all while operating entirely offline.

3. FastAPI is a modern, high-performance Python web framework for building APIs quickly using standard Python type hints. It leverages Pydantic for automatic data validation, serialization, and OpenAPI documentation generation, making it faster than frameworks like Flask while reducing boilerplate code.
